{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d773c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import fashion_mnist # Keras  Fashion-MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import time  # added for timing\n",
    "import pandas as pd\n",
    "from google.colab import files\n",
    "\n",
    "# Load and preprocess data\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train = x_train[..., tf.newaxis]\n",
    "x_test = x_test[..., tf.newaxis]\n",
    "\n",
    "# Define different parameter sets\n",
    "#configs = [...] # (as before) Random choices\n",
    "\n",
    "#after intial testing, the cofigurations have been re-ordered by rank of Test Accuracy\n",
    "configs = [\n",
    "    # Adam (3 versions)\n",
    "    {'filters': 64, 'kernel_size': 5, 'dense_units': 128, 'optimizer': 'adam',    'batch_size': 64, 'learning_rate': 0.001},\n",
    "    {'filters': 32, 'kernel_size': 3, 'dense_units': 128, 'optimizer': 'adam',    'batch_size': 128,'learning_rate': 0.0005},\n",
    "    {'filters': 64, 'kernel_size': 3, 'dense_units': 64,  'optimizer': 'adam',    'batch_size': 64, 'learning_rate': 0.0001},\n",
    "\n",
    "    # RMSprop (3 versions)\n",
    "    {'filters': 64, 'kernel_size': 5, 'dense_units': 128, 'optimizer': 'rmsprop', 'batch_size': 64, 'learning_rate': 0.001},\n",
    "    {'filters': 32, 'kernel_size': 3, 'dense_units': 64,  'optimizer': 'rmsprop', 'batch_size': 64, 'learning_rate': 0.0005},\n",
    "    {'filters': 32, 'kernel_size': 5, 'dense_units': 128, 'optimizer': 'rmsprop', 'batch_size': 128,'learning_rate': 0.0001},\n",
    "\n",
    "    # SGD (3 versions)\n",
    "    {'filters': 64, 'kernel_size': 3, 'dense_units': 128, 'optimizer': 'sgd',     'batch_size': 64, 'learning_rate': 0.01},\n",
    "    {'filters': 32, 'kernel_size': 5, 'dense_units': 128, 'optimizer': 'sgd',     'batch_size': 128,'learning_rate': 0.01},\n",
    "    {'filters': 64, 'kernel_size': 3, 'dense_units': 64,  'optimizer': 'sgd',     'batch_size': 64, 'learning_rate': 0.005},\n",
    "]\n",
    "\n",
    "\n",
    "results = []\n",
    "all_train_acc = []\n",
    "all_val_acc = []\n",
    "\n",
    "# Loop through each config\n",
    "for i, cfg in enumerate(configs, 1):\n",
    "    print(f\"\\nTraining configuration {i}: {cfg}\")\n",
    "\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=x_train.shape[1:]),  # Define the input explicitly\n",
    "        layers.Conv2D(cfg['filters'], (cfg['kernel_size'], cfg['kernel_size']), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),            # pool on a 2x2 basis\n",
    "        layers.Flatten(),                       # flaten to a vector\n",
    "        layers.Dense(cfg['dense_units'], activation='relu'), # array of neurons\n",
    "        layers.Dense(10, activation='softmax')  # one unit per class\n",
    "    ])\n",
    "\n",
    "    # Define optimizer with custom learning rate\n",
    "    if cfg['optimizer'] == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=cfg['learning_rate'])\n",
    "    elif cfg['optimizer'] == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=cfg['learning_rate'])\n",
    "    elif cfg['optimizer'] == 'sgd':\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=cfg['learning_rate'])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {cfg['optimizer']}\")\n",
    "    \n",
    "    model.compile(optimizer=cfg['optimizer'],\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    start_time = time.time()  # Start timing\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        epochs=5,\n",
    "                        batch_size=cfg['batch_size'],\n",
    "                        validation_split=0.1,\n",
    "                        verbose=0)\n",
    "    end_time = time.time()  # End timing\n",
    "    duration = round(end_time - start_time, 2)\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    results.append({\n",
    "        'config': cfg,\n",
    "        'test_accuracy': test_acc,\n",
    "        'train_accuracy': history.history['accuracy'],\n",
    "        'val_accuracy': history.history['val_accuracy'],\n",
    "        'training_time_sec': duration  #  added\n",
    "    })\n",
    "\n",
    "    # Plot training history for each\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "    plt.title(f\"Accuracy (Config {i})\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xticks(range(len(history.history['accuracy'])))\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.ylim(0.68, 0.95)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# All training accuracy curves\n",
    "plt.figure(figsize=(6, 4))\n",
    "for i, r in enumerate(results):\n",
    "    plt.plot(r['train_accuracy'], label=f\"Train {i+1}\")\n",
    "plt.title(\"All Training Accuracy Curves\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.xticks(range(len(results[0]['train_accuracy'])))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0.68, 0.95)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# All validation accuracy curves\n",
    "plt.figure(figsize=(6, 4))\n",
    "for i, r in enumerate(results):\n",
    "    plt.plot(r['val_accuracy'], label=f\"Val {i+1}\")\n",
    "plt.title(\"All Validation Accuracy Curves\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.xticks(range(len(results[0]['val_accuracy'])))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0.68, 0.95)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary = []\n",
    "for i, r in enumerate(results, 1):\n",
    "    summary.append({\n",
    "        'Config #': i,\n",
    "        'Filters': r['config']['filters'],\n",
    "        'Kernel': r['config']['kernel_size'],\n",
    "        'Dense Units': r['config']['dense_units'],\n",
    "        'Optimizer': r['config']['optimizer'],\n",
    "        'Batch Size': r['config']['batch_size'],\n",
    "        'Learning Rate': r['config']['learning_rate'],\n",
    "        'Train Time (s)': r['training_time_sec'],\n",
    "        'Train Acc (final)': round(r['train_accuracy'][-1], 4),\n",
    "        'Val Acc (final)': round(r['val_accuracy'][-1], 4),\n",
    "        'Test Acc': round(r['test_accuracy'], 4)\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"\\nModel Configuration Summary Table:\")\n",
    "display(summary_df)\n",
    "\n",
    "#  Save to CSV and offer download\n",
    "summary_df.to_csv(\"model_summary.csv\", index=False)\n",
    "files.download(\"model_summary.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
